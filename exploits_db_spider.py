#! /usr/bin/env python3.4
# -*- coding: utf-8 -*-

import sys
import urllib.request
import re
import os
import threading
from socket import timeout
import socket
import socks

def get_url(target_str = 'router', start_id = 42740, end_id = 19943):
    url_startwith = "https://www.exploit-db.com/exploits/"
    threads_MAX = 2
    threads_list = []
    range_value = start_id - end_id + 1

    try:
#exploit_id_done.txt, save exploit_id that is done
        save_id = open("exploit_id_done.txt", 'a+')

#read exploit_id_done.txt, for check
        read_ids = open("exploit_id_done.txt", 'r') 
        ids_content = read_ids.read()
        read_ids.close()

        for i in range(range_value):
            current_id = str(start_id - i)

#check this exploit_id. If it's done, pass
            if ids_content.find(current_id) != -1:
                print ('pass', current_id)
                continue

            url = url_startwith + current_id
            try:
                response = urllib.request.urlopen(url, timeout = 30)
                content = response.read().decode("utf-8")

#get the urls that contain 'target_str'
                headline = re.findall(r'<h1 itemprop="headline">(.*?)</h1>', content, re.S)
                if(re.search(r'%s' % target_str, headline[0], re.IGNORECASE)):

#use thread
                    while threading.active_count() > threads_MAX:
                        print ("wait a minute......")
                        time.sleep(10)
                    thread = threading.Thread(get_target(url))
                    threads_list.append(thread)
                    thread.start()
                    for thread in threads_list:
                        thread.join()

#if socket.timeout, save the url into save_timeout_url.txt
            except timeout:
                print ("timeout_error: ", url)
                save_timeout_url(url)

#if UnicodeDecodeError save the url into save_DecodeError_url.txt (It's a pdf)
            except UnicodeDecodeError:
                print ("UnicodeDecodeError: ", url)
                save_id.write(current_id + ', ')
                save_DecodeError_url(url)

            except Exception as e:
                print ('Error--------get_url--------')
                print (e, url)
            else:
                save_id.write(current_id + ', ')
        save_id.close()
    except Exception as e:
        print ('Error--------get_url--------')
        print (e)

# get main info from webpage
def get_info(url):
    info = {}
    response = urllib.request.urlopen(url,timeout=30)
    content = response.read().decode("utf-8")

    try:
        EDB_ID = re.findall(r'<strong>EDB-ID</strong>: (.*?)</td>', content, re.S)
        info['EDB_ID'] = EDB_ID[0]
        Author = re.findall(r'<strong>Author</strong>: <a href=".*?">(.*?)</a>', content, re.S)
        info['Author'] = Author[0]
        Published = re.findall(r'<strong>Published</strong>: (.*?)</td>', content, re.S)
        info['Published'] = Published[0]
        CVE = re.findall(r'(CVE-\d*-\d*)</a>', content, re.S)

#if don't exist CVE, save 'N/A'
        if len(CVE) == 0:
            CVE.append('N/A')
        info['CVE'] = CVE[0]
        Platform = re.findall(r'<strong>Platform</strong>: <a href=".*?">(.*?)</a>', content, re.S)
        info['Platform'] = Platform[0]
        Exploit_url = re.findall(r'<strong>Exploit</strong>.*?<a href="(.*?)">', content, re.S)
        info['Exploit_url'] = Exploit_url[0]
    except Exception as e:
        print ('Error--------get_info--------\n', e)

    return info

#save exploit's info into EDB_ID.txt
def save_info(**info):
    filename = info['EDB_ID'] + '.txt'
    print ('********saving_%s********'% (filename))
    filepath = './info/' + filename
    try:
        info_file = open(filepath, 'wb')
        for (k, v) in info.items():
            line = k + ': ' + v + '\r\n'
            info_file.write(bytes(line, 'UTF-8'))
        info_file.close()
    except Exception as e:
        print (e)

#download exploit by url of exploit
def download_exploit(url):
    print ('********downloading_exploit*********')
    print ('exploit: ', url)
    split_url = url.split('/')
    filename = split_url[-1]
    try:
        filepath = './exploits/' + filename
        save_exploit = open(filepath, 'wb')
        response = urllib.request.urlopen(url, timeout = 30)
        content = response.read()
        save_exploit.write(content)
        save_exploit.close()
    except Exception as e:
        print ('Error--------download_exploit--------\n', e)

#get router's url, save info, download exploit
def get_target(url):
    info = get_info(url)
    save_info(**info)
    Exploit_url = info['Exploit_url']
    download_exploit(Exploit_url)

def save_timeout_url(url):
    try:
        save_url = open("save_timeout_url.txt", 'wb')
        save_url.write(bytes(url + '\r\n', 'UTF-8'))
        save_url.close()
    except Exception as e:
        print ('Error--------save_timeout_url--------\n', e)

def save_DecodeError_url(url):
    try:
        save_url = open("save_DecodeError_url.txt", 'ab+')
        save_url.write(bytes(url + '\r\n', 'UTF-8'))
        save_url.close()
    except Exception as e:
        print ('Error--------save_DecodeError_url--------\n', e)

#use proxy
def use_proxy():
    SOCKS5_PROXY_HOST = '127.0.0.1'
    SOCKS5_PROXY_PORT = 1080  # socks 代理本地端口
    socks.set_default_proxy(socks.SOCKS5, SOCKS5_PROXY_HOST, SOCKS5_PROXY_PORT)
    socks.set_default_timeout(30)
    socket.socket = socks.socksocket

#make sure target dirs are existed
def env():
    try:
        if not os.path.exists('info'):
            os.mkdir('info')
        if not os.path.exists('exploits'):
            os.mkdir('exploits')
    except Exception as e:
        print ('Error--------env--------\n', e)

def main():
    help_info = '''Usage:\nOption1 - router_exploits_spider.py\nOption2 - router_exploits_spider.py target_str start_id end_id'''
    if((len(sys.argv) != 1) & (len(sys.argv) != 4)):
        print (help_info)
    elif(len(sys.argv) == 1):
        print('<<<<<<<<<<<<<<<<<<<<START>>>>>>>>>>>>>>>>>>>>')
        env()
#        use_proxy()
        get_url()
        print('<<<<<<<<<<<<<<<<<<<<<END>>>>>>>>>>>>>>>>>>>>>')
    else:
        print('<<<<<<<<<<<<<<<<<<<<START>>>>>>>>>>>>>>>>>>>>')
        env()
#        use_proxy()
        get_url(sys.argv[1], int(sys.argv[2]), int(sys.argv[3]))
        print('<<<<<<<<<<<<<<<<<<<<<END>>>>>>>>>>>>>>>>>>>>>')

if __name__ == "__main__":
    main()
